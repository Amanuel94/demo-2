{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg+++TWsukoq9Wal+DjtRS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanuel94/demo-2/blob/master/view_balancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Gathering Datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "mjTzq1-K79r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import json\n",
        "import itertools"
      ],
      "metadata": {
        "id": "CtAldUZj9BUZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Fake news challenge ([link](https://www.kaggle.com/datasets/abhinavkrjha/fake-news-challenge))"
      ],
      "metadata": {
        "id": "KmqEPvF98-pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://storage.googleapis.com/kaggle-data-sets/1249857/2084561/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230305%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230305T183036Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=803f4dc914566d9b2f5dc5496faec02ffc2c3004753aaea5140784bc7cb77ebd5a02b2f0d59f55770f3b72fcf2c94b326cffc62000d5d58078cceda2c9f6e4e7ec5b634bc50075a2938dd7456a1de56dfd998d3cc58031b7986ec2e1b4afdf80937f2d826b1fbbcda56b4b2d4a0ae59b84518b49974162e50d0e032928c802a5850837685c9ec5677a3d78ea78e025d4f90a36a4d5ad849e7c4ee144280f27b3eaee3353b9d9a05b33e9b0999d1d9b99bff6bd40ce1b76fae0097441417bc0a505cc12826b9c98dc8c2569e7ef637c79f630b42cc007dcc1d9fbb103647c6ef5910e87b2549e5f9e08e6cd76d56b45e31bf7eb8e38d1db215a39d41230accdbf\"\n",
        "name = \"fake_news.zip\"\n",
        "urllib.request.urlretrieve(url, name)\n",
        "\n",
        "!mkdir fake_news\n",
        "\n",
        "with zipfile.ZipFile(\"/content/fake_news.zip\", 'r') as f:\n",
        "  f.extractall('./fake_news')"
      ],
      "metadata": {
        "id": "QWJg9sjr87rL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bodies = pd.read_csv('./fake_news/train_bodies.csv')\n",
        "train_stances = pd.read_csv('./fake_news/train_stances.csv')\n",
        "\n",
        "train_pairs = pd.merge(train_bodies, train_stances, on = 'Body ID')\n",
        "train_ = train_pairs[train_pairs['Stance'] == 'disagree']\n",
        "\n",
        "train_.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkLD3-sIAxSb",
        "outputId": "608026e2-1eb7-444b-9111-157a5cf711b8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(840, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "support = []\n",
        "deny = []\n",
        "\n",
        "for i in range(train_pairs.shape[0]):\n",
        "  if train_pairs.iloc[i]['Stance'] == \"agree\":\n",
        "    support.append([train_pairs.iloc[i]['Body ID'], train_pairs.iloc[i]['Headline']])\n",
        "\n",
        "  if train_pairs.iloc[i]['Stance'] == \"disagree\":\n",
        "    deny.append([train_pairs.iloc[i]['Body ID'], train_pairs.iloc[i]['Headline']])\n",
        "\n",
        "support_df = pd.DataFrame(support).rename(columns = {0: \"Body ID\", 1:\"support\"})\n",
        "deny_df = pd.DataFrame(deny).rename(columns = {0: \"Body ID\", 1:\"deny\"})\n",
        "merged_df = pd.merge(support_df, deny_df, on = 'Body ID', how = \"inner\")\n",
        "merged_df.drop(['Body ID'], axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "z5cNTvE2E0uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcZ6XUxFI0ry",
        "outputId": "384b9294-9c05-48fe-f3d6-a3ed440cdcc0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3580, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir gathered_data\n",
        "train_.to_csv(\"./gathered_data/fake-news-challenge.csv\")\n",
        "merged_df.to_csv(\"./gathered_data/fake-news-challenge-extended.csv\")\n"
      ],
      "metadata": {
        "id": "n3ajS2XDKr81"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 RumorEval2019 ([link to paper](https://www.researchgate.net/publication/345434708_SemEval-2019_Task_7_RumourEval_Determining_Rumour_Veracity_and_Support_for_Rumours))"
      ],
      "metadata": {
        "id": "L3Khp9Y3LZNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kochkinaelena/RumourEval2019.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKnk12CkLvH_",
        "outputId": "87b9f76d-d3c1-4ffb-8afd-26b414ce822f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RumourEval2019'...\n",
            "remote: Enumerating objects: 147, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 147 (delta 1), reused 6 (delta 0), pack-reused 140\u001b[K\n",
            "Receiving objects: 100% (147/147), 14.96 MiB | 16.65 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://figshare.com/ndownloader/files/16188500'\n",
        "filename = 'rumoureval2019.tar.bz2'\n",
        "\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "with tarfile.open(filename, 'r:bz2') as tar_ref:\n",
        "    tar_ref.extractall('.')\n",
        "\n",
        "with zipfile.ZipFile('/content/rumoureval2019/rumoureval-2019-training-data.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('.')\n",
        "\n",
        "!mv /content/RumourEval2019/preprocessing /content\n",
        "os.chdir('./preprocessing/')\n"
      ],
      "metadata": {
        "id": "mTzjHk9JP8ri"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is taken from the `preprocecssing_tweets.py` file found in the referenced github repository"
      ],
      "metadata": {
        "id": "LhEohBz6bEtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tree2branches import tree2branches\n",
        "def load_true_labels():\n",
        "\n",
        "    tweet_label_dict = {}\n",
        "    veracity_label_dict = {}\n",
        "    path_dev = \"../rumoureval-2019-training-data/dev-key.json\"\n",
        "    with open(path_dev, 'r') as f:\n",
        "        dev_key = json.load(f)\n",
        "\n",
        "    path_train = \"../rumoureval-2019-training-data/train-key.json\"\n",
        "    with open(path_train, 'r') as f:\n",
        "        train_key = json.load(f)\n",
        "\n",
        "    tweet_label_dict['dev'] = dev_key['subtaskaenglish']\n",
        "    tweet_label_dict['train'] = train_key['subtaskaenglish']\n",
        "\n",
        "    return tweet_label_dict\n",
        "def load_dataset():\n",
        "\n",
        "    # Load labels and split for task A and task B\n",
        "    tweet_label_dict = load_true_labels()\n",
        "    print(tweet_label_dict)\n",
        "    dev = tweet_label_dict['dev']\n",
        "    train = tweet_label_dict['train']\n",
        "    dev_tweets = dev.keys()\n",
        "    train_tweets = train.keys()\n",
        "    # Load folds and conversations\n",
        "    path_to_folds = '../rumoureval-2019-training-data/twitter-english'\n",
        "\n",
        "    folds = sorted(os.listdir(path_to_folds))\n",
        "    newfolds = [i for i in folds if i[0] != '.']\n",
        "    folds = newfolds\n",
        "    cvfolds = {}\n",
        "    allconv = []\n",
        "    train_dev_split = {}\n",
        "    train_dev_split['dev'] = []\n",
        "    train_dev_split['train'] = []\n",
        "    train_dev_split['test'] = []\n",
        "\n",
        "    # folds are tweet titles\n",
        "    for nfold, fold in enumerate(folds):\n",
        "        path_to_tweets = os.path.join(path_to_folds, fold)\n",
        "        tweet_data = sorted(os.listdir(path_to_tweets))\n",
        "        newfolds = [i for i in tweet_data if i[0] != '.']\n",
        "        tweet_data = newfolds\n",
        "        conversation = {}\n",
        "        # tweet_data is the thread of tweet\n",
        "        for foldr in tweet_data:\n",
        "            flag = 0\n",
        "            conversation['id'] = foldr\n",
        "            tweets = []\n",
        "            path_repl = path_to_tweets+'/'+foldr+'/replies'\n",
        "            files_t = sorted(os.listdir(path_repl))\n",
        "            newfolds = [i for i in files_t if i[0] != '.']\n",
        "            files_t = newfolds\n",
        "\n",
        "            if files_t != []:\n",
        "                for repl_file in files_t:\n",
        "                    with open(os.path.join(path_repl, repl_file)) as f:\n",
        "                        for line in f:\n",
        "                            tw = json.loads(line)\n",
        "                            tw['used'] = 0\n",
        "                            replyid = tw['id_str']\n",
        "                            if replyid in dev_tweets:\n",
        "                                tw['set'] = 'dev'\n",
        "                                tw['label'] = dev[replyid]\n",
        "        #                        train_dev_tweets['dev'].append(tw)\n",
        "                                if flag == 'train':\n",
        "                                    print(\"The tree is split between sets\", foldr)\n",
        "                                flag = 'dev'\n",
        "                            elif replyid in train_tweets:\n",
        "                                tw['set'] = 'train'\n",
        "                                tw['label'] = train[replyid]\n",
        "        #                        train_dev_tweets['train'].append(tw)\n",
        "                                if flag == 'dev':\n",
        "                                    print(\"The tree is split between sets\", foldr)\n",
        "                                flag = 'train'\n",
        "                            else:\n",
        "                                print(\"Tweet was not found! ID: \", foldr)\n",
        "                            tweets.append(tw)\n",
        "                            if tw['text'] is None:\n",
        "                                print(\"Tweet has no text\", tw['id'])\n",
        "                conversation['replies'] = tweets\n",
        "\n",
        "                path_src = path_to_tweets+'/'+foldr+'/source-tweet'\n",
        "                files_t = sorted(os.listdir(path_src))\n",
        "                with open(os.path.join(path_src, files_t[0])) as f:\n",
        "                    for line in f:\n",
        "                        src = json.loads(line)\n",
        "                        src['used'] = 0\n",
        "                        scrcid = src['id_str']\n",
        "                        src['set'] = flag\n",
        "                        src['label'] = tweet_label_dict[flag][scrcid]\n",
        "\n",
        "                conversation['source'] = src\n",
        "                if src['text'] is None:\n",
        "                    print(\"Tweet has no text\", src['id'])\n",
        "                path_struct = path_to_tweets+'/'+foldr+'/structure.json'\n",
        "                with open(path_struct) as f:\n",
        "                    for line in f:\n",
        "                        struct = json.loads(line)\n",
        "                if len(struct) > 1:\n",
        "                    # I had to alter the structure of this conversation\n",
        "                    if foldr == '553480082996879360':\n",
        "                        new_struct = {}\n",
        "                        new_struct[foldr] = struct[foldr]\n",
        "                        new_struct[foldr]['553495625527209985'] = struct['553485679129534464']['553495625527209985']\n",
        "                        new_struct[foldr]['553495937432432640'] = struct['553490097623269376']['553495937432432640']\n",
        "                        struct = new_struct\n",
        "                    else:\n",
        "                        new_struct = {}\n",
        "                        new_struct[foldr] = struct[foldr]\n",
        "                        struct = new_struct\n",
        "                    # Take item from structure if key is same as source tweet id\n",
        "                conversation['structure'] = struct\n",
        "\n",
        "                branches = tree2branches(conversation['structure'])\n",
        "                conversation['branches'] = branches\n",
        "                train_dev_split[flag].append(conversation.copy())\n",
        "                allconv.append(conversation.copy())\n",
        "            else:\n",
        "                flag = 'train'\n",
        "                path_src = path_to_tweets+'/'+foldr+'/source-tweet'\n",
        "                files_t = sorted(os.listdir(path_src))\n",
        "                with open(os.path.join(path_src, files_t[0])) as f:\n",
        "                    for line in f:\n",
        "                        src = json.loads(line)\n",
        "                        src['used'] = 0\n",
        "                        scrcid = src['id_str']\n",
        "                        src['set'] = flag\n",
        "                        src['label'] = tweet_label_dict[flag][scrcid]\n",
        "\n",
        "                conversation['source'] = src\n",
        "                if src['text'] is None:\n",
        "                    print(\"Tweet has no text\", src['id'])\n",
        "\n",
        "                path_struct = path_to_tweets+'/'+foldr+'/structure.json'\n",
        "                with open(path_struct) as f:\n",
        "                    for line in f:\n",
        "                        struct = json.loads(line)\n",
        "                if len(struct) > 1:\n",
        "                    # print \"Structure has more than one root\"\n",
        "                    new_struct = {}\n",
        "                    new_struct[foldr] = struct[foldr]\n",
        "                    struct = new_struct\n",
        "                    # Take item from structure if key is same as source tweet id\n",
        "                conversation['structure'] = struct\n",
        "                branches = tree2branches(conversation['structure'])\n",
        "\n",
        "                conversation['branches'] = branches\n",
        "                train_dev_split[flag].append(conversation.copy())\n",
        "                allconv.append(conversation.copy())\n",
        "\n",
        "                print(foldr)\n",
        "\n",
        "        cvfolds[fold] = allconv\n",
        "        allconv = []\n",
        "\n",
        "    return train_dev_split"
      ],
      "metadata": {
        "id": "XJxUXwMOQVST"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data_dev  = load_dataset()['dev']\n",
        "tweet_data_train = load_dataset()['train']\n",
        "\n",
        "source_stance = []\n",
        "for tweet in tweet_data_train:\n",
        "  for rep in tweet['replies']:\n",
        "    if rep['label'] == 'deny':\n",
        "      source_stance.append([tweet['source']['text'], rep['text']])\n",
        "\n",
        "for tweet in tweet_data_dev:\n",
        "  for rep in tweet['replies']:\n",
        "    if rep['label'] == 'deny':\n",
        "      source_stance.append([tweet['source']['text'], rep['text']])\n",
        "\n",
        "tweet_arr = np.array(source_stance)\n",
        "tweet_df = pd.DataFrame(tweet_arr)\n",
        "\n",
        "tweet_df.to_csv('../gathered_data/rumour-eval-2019.csv', index = False)"
      ],
      "metadata": {
        "id": "JXzenis2Rb_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above csv file contains two columns: one the source tweet, the other contradicting replies to that tweet. Another way of processing the data is provided below"
      ],
      "metadata": {
        "id": "VK6IS1N1SRoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_stance_extended = []\n",
        "for tweet in tweet_data_train:\n",
        "  support = []\n",
        "  deny = []\n",
        "  for rep in tweet['replies']:\n",
        "    if rep['label'] == 'deny':\n",
        "      deny.append(rep['text'])\n",
        "    if rep['label'] == 'support':\n",
        "      support.append(tweet['source']['text']+\". \"+rep['text'])\n",
        "\n",
        "  prod = itertools.product(support, deny)\n",
        "  for tup in prod:\n",
        "    source_stance_extended.append(list(tup))\n",
        "\n",
        "\n",
        "for tweet in tweet_data_dev:\n",
        "  support = []\n",
        "  deny = []\n",
        "  for rep in tweet['replies']:\n",
        "    if rep['label'] == 'deny':\n",
        "      deny.append(rep['text'])\n",
        "    if rep['label'] == 'support':\n",
        "      support.append(tweet['source']['text']+\". \" + rep['text'])\n",
        "\n",
        "    prod = itertools.product(support, deny)\n",
        "  \n",
        "  for tup in prod:\n",
        "    source_stance_extended.append(list(tup))\n",
        "    \n",
        "extended_df = pd.DataFrame(source_stance_extended)\n",
        "extended_num = np.array(source_stance_extended)\n",
        "extended_df = pd.DataFrame(extended_num)\n",
        "\n",
        "tweet_df.to_csv('../gathered_data/rumour-eval-2019-extended.csv', index = False)"
      ],
      "metadata": {
        "id": "Pez7h7fQSD9Z"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('../')"
      ],
      "metadata": {
        "id": "oeWKs6PNSqfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Perspectrum ([link](https://github.com/CogComp/perspectrum))\n"
      ],
      "metadata": {
        "id": "31i4967NThXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CogComp/perspectrum.git\n",
        "\n",
        "path  = \"/content/perspectrum/data/dataset/\"\n",
        "with open(path + \"perspectrum_with_answers_v1.0.json\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "with open(path + \"perspective_pool_v1.0.json\") as f:\n",
        "  perspectives = json.load(f)"
      ],
      "metadata": {
        "id": "ifP-VAsQTgGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findByPids(pids, perspectives):\n",
        "  ans = []\n",
        "  for per in perspectives:\n",
        "    if per['pId'] in pids:\n",
        "      ans.append(per['text'])\n",
        "\n",
        "  return ans"
      ],
      "metadata": {
        "id": "ejFgty93UWPe"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_arr = []\n",
        "for data in dataset:\n",
        "  opp  = set()\n",
        "  for pers in data['perspectives']:\n",
        "    \n",
        "    if pers['stance_label_3'] == \"UNDERMINE\":\n",
        "      opp =  opp.union(set(pers['pids'])) \n",
        "  if opp:  \n",
        "    data_arr.append([data['text'], list(opp)])\n",
        "\n",
        "data_ = []\n",
        "for data in data_arr:\n",
        "  undermine = findByPids(data[1], perspectives)\n",
        "\n",
        "  for pers in undermine:\n",
        "    data_.append([data[0], pers])\n",
        "\n",
        "\n",
        "persp_df = pd.DataFrame(data_)\n",
        "persp_df.to_csv('./gathered_data/perspectrum.csv', index = False)"
      ],
      "metadata": {
        "id": "huReJQnNUJYE"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extended_arr = []\n",
        "for data in dataset:\n",
        "  opp  = set()\n",
        "  sim = set()\n",
        "  for pers in data['perspectives']:\n",
        "    if pers['stance_label_3'] == \"UNDERMINE\":\n",
        "      opp =  opp.union(set(pers['pids'])) \n",
        "    if pers['stance_label_3'] == \"SUPPORT\":\n",
        "      sim = sim.union(set(pers['pids']))\n",
        "  if opp:  \n",
        "    extended_arr.append([data['text'], list(opp), list(sim)])\n",
        "\n",
        "expanded = []\n",
        "for data in extended_arr:\n",
        "\n",
        "  undermine = findByPids(data[1], perspectives)\n",
        "  support  = findByPids(data[2],  perspectives)\n",
        "\n",
        "  for pers in undermine:\n",
        "    expanded.append([data[0], pers])\n",
        "\n",
        "  prod = itertools.product(support, undermine)\n",
        "\n",
        "  for tup in prod:\n",
        "    expanded.append(list(tup))\n",
        "\n",
        "extended_df = pd.DataFrame(expanded)\n",
        "extended_df.to_csv('./gathered_data/perspectrum-extended.csv', index = False)"
      ],
      "metadata": {
        "id": "6ipfrzm7U7fb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 paper-with-code.com ([link](https://paperswithcode.com/task/stance-detection))\n",
        "\n"
      ],
      "metadata": {
        "id": "eWDx579bVomb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 VAST"
      ],
      "metadata": {
        "id": "ovDkd-hBWJ8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/emilyallaway/zero-shot-stance.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELs4WIOBV5U2",
        "outputId": "988024b0-7a7d-4201-ee58-f5423f81db28"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'zero-shot-stance'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 93 (delta 5), reused 3 (delta 3), pack-reused 69\u001b[K\n",
            "Unpacking objects: 100% (93/93), 63.59 MiB | 7.23 MiB/s, done.\n",
            "Updating files: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VAST_df = pd.read_csv(\"/content/zero-shot-stance/data/VAST/vast_train.csv\")\n",
        "\n",
        "support = []\n",
        "deny = []\n",
        "for row in range(VAST_df.shape[0]):\n",
        "  if VAST_df.iloc[row]['label'] == 0:\n",
        "    deny.append([VAST_df.iloc[row]['new_topic'], VAST_df.iloc[row]['post']])\n",
        "  if VAST_df.iloc[row]['label'] == 1:\n",
        "    support.append([VAST_df.iloc[row]['new_topic'], VAST_df.iloc[row]['post']]) \n",
        "\n",
        "support_df = pd.DataFrame(support)\n",
        "deny_df = pd.DataFrame(deny)\n",
        "\n",
        "support_df = support_df.rename(columns = {0:\"topic\", 1:\"support\"})\n",
        "deny_df = deny_df.rename(columns = {0:\"topic\", 1:\"deny\"})\n",
        "\n",
        "merged_df = pd.merge(support_df, deny_df, on = \"topic\", how = 'inner').drop_duplicates()\n",
        "merged_df.to_csv('./gathered_data/vast.csv')\n"
      ],
      "metadata": {
        "id": "sc7xYZJ7WjN5"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 OpenStance"
      ],
      "metadata": {
        "id": "vNBcxDm1YcgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/xhz0809/OpenStance.git\n",
        "\n",
        "sem_df = pd.read_csv(\"/content/OpenStance/data/SemT6/original_dataset/trainingdata-all-annotations.txt\",delimiter = '\\t')\n",
        "\n",
        "support = []\n",
        "deny = []\n",
        "for row in range(sem_df.shape[0]):\n",
        "  if sem_df.iloc[row]['Stance'] == 'AGAINST':\n",
        "    deny.append([sem_df.iloc[row]['Target'], sem_df.iloc[row]['Tweet']])\n",
        "  if sem_df.iloc[row]['Stance'] == 'FAVOR':\n",
        "    support.append([sem_df.iloc[row]['Target'], sem_df.iloc[row]['Tweet']]) \n",
        "\n",
        "support_df = pd.DataFrame(support)\n",
        "deny_df = pd.DataFrame(deny)\n",
        "\n",
        "support_df = support_df.rename(columns = {0:\"topic\", 1:\"support\"})\n",
        "deny_df = deny_df.rename(columns = {0:\"topic\", 1:\"deny\"})\n",
        "\n",
        "merged_df = pd.merge(support_df, deny_df, on = \"topic\", how = 'inner').drop_duplicates()\n",
        "\n",
        "merged_df.to_csv('./gathered_data/semt6.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImtbZhMNXG65",
        "outputId": "fac8ace8-4063-49ba-aa6e-a8a9553547d9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OpenStance'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 153 (delta 8), reused 0 (delta 0), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (153/153), 10.29 MiB | 5.05 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.common import file_exists\n",
        "dir = '/content/gathered_data'\n",
        "file_ = '/content/gathered_data.zip'\n",
        "\n",
        "with zipfile.ZipFile(file_, 'w') as zip_object:\n",
        "    \n",
        "    for foldername, subfolders, filenames in os.walk(dir):        \n",
        "        for filename in filenames:          \n",
        "            file_path = os.path.join(foldername, filename)\n",
        "        \n",
        "            zip_object.write(file_path)\n"
      ],
      "metadata": {
        "id": "vLuGoNETZmkc"
      },
      "execution_count": 68,
      "outputs": []
    }
  ]
}